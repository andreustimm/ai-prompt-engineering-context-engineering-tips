"""
Memory/Conversation (MemÃ³ria/Conversa)

TÃ©cnicas para manter contexto e memÃ³ria de conversa atravÃ©s de
mÃºltiplas interaÃ§Ãµes com um LLM.

Tipos de MemÃ³ria:
- Buffer Memory: Armazena histÃ³rico completo de conversa
- Window Memory: Armazena Ãºltimas N trocas
- Summary Memory: Armazena histÃ³rico resumido
- Entity Memory: Rastreia entidades mencionadas

Casos de uso:
- Chatbots e assistentes virtuais
- DiÃ¡logos de mÃºltiplos turnos
- InteraÃ§Ãµes personalizadas
- Respostas conscientes de contexto
"""

import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from typing import Optional
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from config import (
    get_llm,
    TokenUsage,
    extract_tokens_from_response,
    print_token_usage,
    print_total_usage
)

# Rastreador global de tokens
token_tracker = TokenUsage()


class MemoriaBuffer:
    """
    MemÃ³ria buffer simples que armazena histÃ³rico completo de conversa.
    Boa para conversas curtas onde contexto completo Ã© importante.
    """

    def __init__(self, max_mensagens: int = 100):
        self.mensagens: list = []
        self.max_mensagens = max_mensagens

    def adicionar_mensagem_usuario(self, conteudo: str):
        """Adiciona uma mensagem do usuÃ¡rio ao histÃ³rico."""
        self.mensagens.append(HumanMessage(content=conteudo))
        self._cortar_se_necessario()

    def adicionar_mensagem_ia(self, conteudo: str):
        """Adiciona uma mensagem da IA ao histÃ³rico."""
        self.mensagens.append(AIMessage(content=conteudo))
        self._cortar_se_necessario()

    def _cortar_se_necessario(self):
        """MantÃ©m apenas as mensagens mais recentes se limite excedido."""
        if len(self.mensagens) > self.max_mensagens:
            self.mensagens = self.mensagens[-self.max_mensagens:]

    def obter_mensagens(self) -> list:
        """ObtÃ©m todas as mensagens."""
        return self.mensagens.copy()

    def limpar(self):
        """Limpa todas as mensagens."""
        self.mensagens = []


class MemoriaJanela:
    """
    MemÃ³ria de janela que armazena apenas as Ãºltimas K trocas.
    Boa para conversas longas onde contexto recente importa mais.
    """

    def __init__(self, tamanho_janela: int = 5):
        self.mensagens: list = []
        self.tamanho_janela = tamanho_janela  # NÃºmero de trocas (pares usuÃ¡rio + IA)

    def adicionar_troca(self, mensagem_usuario: str, mensagem_ia: str):
        """Adiciona uma troca completa."""
        self.mensagens.append(HumanMessage(content=mensagem_usuario))
        self.mensagens.append(AIMessage(content=mensagem_ia))
        self._cortar_janela()

    def _cortar_janela(self):
        """MantÃ©m apenas mensagens dentro da janela."""
        max_mensagens = self.tamanho_janela * 2  # 2 mensagens por troca
        if len(self.mensagens) > max_mensagens:
            self.mensagens = self.mensagens[-max_mensagens:]

    def obter_mensagens(self) -> list:
        return self.mensagens.copy()

    def limpar(self):
        self.mensagens = []


class MemoriaResumo:
    """
    MemÃ³ria que mantÃ©m um resumo contÃ­nuo da conversa.
    Boa para conversas muito longas para prevenir estouro de tokens.
    """

    def __init__(self):
        self.resumo: str = ""
        self.mensagens_recentes: list = []
        self.max_recentes = 4  # MantÃ©m Ãºltimas 4 mensagens para contexto

    def adicionar_troca(self, mensagem_usuario: str, mensagem_ia: str):
        """Adiciona uma troca e potencialmente atualiza resumo."""
        self.mensagens_recentes.append(HumanMessage(content=mensagem_usuario))
        self.mensagens_recentes.append(AIMessage(content=mensagem_ia))

        # Se muitas mensagens recentes, incorporar no resumo
        if len(self.mensagens_recentes) > self.max_recentes:
            self._atualizar_resumo()

    def _atualizar_resumo(self):
        """Atualiza o resumo com mensagens antigas."""
        llm = get_llm(temperature=0)

        # Pega mensagens mais antigas para resumir
        para_resumir = self.mensagens_recentes[:-self.max_recentes]

        if not para_resumir:
            return

        contexto = "\n".join([
            f"{'UsuÃ¡rio' if isinstance(m, HumanMessage) else 'Assistente'}: {m.content}"
            for m in para_resumir
        ])

        prompt = ChatPromptTemplate.from_messages([
            ("system", """Resuma a seguinte conversa, mantendo informaÃ§Ãµes chave
e quaisquer detalhes ou decisÃµes importantes. Seja conciso mas abrangente."""),
            ("user", """Resumo anterior (se houver):
{resumo_anterior}

Nova conversa para incorporar:
{conversa}

Resumo atualizado:""")
        ])

        chain = prompt | llm
        response = chain.invoke({
            "resumo_anterior": self.resumo or "Nenhum",
            "conversa": contexto
        })

        input_tokens, output_tokens = extract_tokens_from_response(response)
        token_tracker.add(input_tokens, output_tokens)

        self.resumo = response.content
        self.mensagens_recentes = self.mensagens_recentes[-self.max_recentes:]

    def obter_contexto(self) -> dict:
        """ObtÃ©m resumo e mensagens recentes."""
        return {
            "resumo": self.resumo,
            "mensagens_recentes": self.mensagens_recentes.copy()
        }

    def limpar(self):
        self.resumo = ""
        self.mensagens_recentes = []


class MemoriaEntidades:
    """
    MemÃ³ria que rastreia entidades mencionadas na conversa.
    Boa para manter conhecimento sobre pessoas, lugares, coisas discutidas.
    """

    def __init__(self):
        self.entidades: dict = {}  # nome_entidade -> descriÃ§Ã£o/info

    def extrair_e_atualizar_entidades(self, texto: str):
        """Extrai entidades do texto e atualiza memÃ³ria."""
        llm = get_llm(temperature=0)

        prompt = ChatPromptTemplate.from_messages([
            ("system", """Extraia entidades nomeadas do texto. Para cada entidade,
forneÃ§a uma breve descriÃ§Ã£o baseada no que foi mencionado.
Retorne como JSON: {"nome_entidade": "descriÃ§Ã£o", ...}
Inclua apenas entidades claramente nomeadas (pessoas, lugares, organizaÃ§Ãµes, produtos)."""),
            ("user", "{texto}")
        ])

        chain = prompt | llm
        response = chain.invoke({"texto": texto})

        input_tokens, output_tokens = extract_tokens_from_response(response)
        token_tracker.add(input_tokens, output_tokens)

        # Analisa resposta
        try:
            import json
            conteudo = response.content
            inicio = conteudo.find('{')
            fim = conteudo.rfind('}') + 1
            if inicio >= 0 and fim > inicio:
                entidades = json.loads(conteudo[inicio:fim])
                self.entidades.update(entidades)
        except Exception:
            pass

    def obter_info_entidade(self, nome_entidade: str) -> Optional[str]:
        """ObtÃ©m informaÃ§Ã£o sobre uma entidade especÃ­fica."""
        return self.entidades.get(nome_entidade.lower())

    def obter_todas_entidades(self) -> dict:
        """ObtÃ©m todas as entidades rastreadas."""
        return self.entidades.copy()

    def limpar(self):
        self.entidades = {}


class CadeiaConversa:
    """
    Cadeia de conversa com memÃ³ria configurÃ¡vel.
    """

    def __init__(self, tipo_memoria: str = "buffer", prompt_sistema: str = None):
        self.tipo_memoria = tipo_memoria
        self.prompt_sistema = prompt_sistema or "VocÃª Ã© um assistente Ãºtil."

        # Inicializa memÃ³ria apropriada
        if tipo_memoria == "buffer":
            self.memoria = MemoriaBuffer()
        elif tipo_memoria == "janela":
            self.memoria = MemoriaJanela(tamanho_janela=5)
        elif tipo_memoria == "resumo":
            self.memoria = MemoriaResumo()
        else:
            self.memoria = MemoriaBuffer()

    def conversar(self, entrada_usuario: str) -> str:
        """Processa entrada do usuÃ¡rio e gera resposta."""
        llm = get_llm(temperature=0.7)

        # ConstrÃ³i lista de mensagens
        mensagens = [SystemMessage(content=self.prompt_sistema)]

        if self.tipo_memoria == "resumo":
            contexto = self.memoria.obter_contexto()
            if contexto["resumo"]:
                mensagens.append(SystemMessage(content=f"Resumo da conversa: {contexto['resumo']}"))
            mensagens.extend(contexto["mensagens_recentes"])
        else:
            mensagens.extend(self.memoria.obter_mensagens())

        mensagens.append(HumanMessage(content=entrada_usuario))

        # ObtÃ©m resposta
        response = llm.invoke(mensagens)

        input_tokens, output_tokens = extract_tokens_from_response(response)
        token_tracker.add(input_tokens, output_tokens)
        print_token_usage(input_tokens, output_tokens)

        resposta_ia = response.content

        # Atualiza memÃ³ria
        if self.tipo_memoria == "buffer":
            self.memoria.adicionar_mensagem_usuario(entrada_usuario)
            self.memoria.adicionar_mensagem_ia(resposta_ia)
        elif self.tipo_memoria == "janela":
            self.memoria.adicionar_troca(entrada_usuario, resposta_ia)
        elif self.tipo_memoria == "resumo":
            self.memoria.adicionar_troca(entrada_usuario, resposta_ia)

        return resposta_ia

    def resetar(self):
        """Reseta memÃ³ria de conversa."""
        self.memoria.limpar()


def demo_memoria_buffer():
    """Demonstra memÃ³ria buffer."""
    print("\nğŸ“š DEMO MEMÃ“RIA BUFFER")
    print("-" * 40)

    cadeia = CadeiaConversa(
        tipo_memoria="buffer",
        prompt_sistema="VocÃª Ã© um conselheiro de viagens Ãºtil."
    )

    trocas = [
        "OlÃ¡! Estou planejando uma viagem ao JapÃ£o.",
        "Qual Ã© a melhor Ã©poca para visitar?",
        "Quais sÃ£o os lugares imperdÃ­veis em TÃ³quio?",
        "Qual foi a primeira coisa que mencionei?"
    ]

    for msg_usuario in trocas:
        print(f"\nğŸ‘¤ UsuÃ¡rio: {msg_usuario}")
        resposta = cadeia.conversar(msg_usuario)
        print(f"ğŸ¤– Assistente: {resposta[:300]}...")


def demo_memoria_janela():
    """Demonstra memÃ³ria de janela."""
    print("\nğŸ“š DEMO MEMÃ“RIA JANELA (janela=3)")
    print("-" * 40)

    cadeia = CadeiaConversa(
        tipo_memoria="janela",
        prompt_sistema="VocÃª Ã© um assistente de culinÃ¡ria Ãºtil."
    )

    trocas = [
        "Quero fazer macarrÃ£o.",
        "Quais ingredientes preciso para carbonara?",
        "Como cozinho o macarrÃ£o perfeitamente?",
        "E sobre o molho?",
        "O que eu originalmente queria fazer?"  # Testa se contexto inicial foi perdido
    ]

    for msg_usuario in trocas:
        print(f"\nğŸ‘¤ UsuÃ¡rio: {msg_usuario}")
        resposta = cadeia.conversar(msg_usuario)
        print(f"ğŸ¤– Assistente: {resposta[:300]}...")


def demo_memoria_entidades():
    """Demonstra memÃ³ria de entidades."""
    print("\nğŸ“š DEMO MEMÃ“RIA DE ENTIDADES")
    print("-" * 40)

    memoria_entidades = MemoriaEntidades()

    textos = [
        "Acabei de comeÃ§ar a trabalhar no Google em SÃ£o Paulo. Minha gerente Ã© Sarah Chen.",
        "Sarah mencionou que nossa equipe estÃ¡ trabalhando em um novo projeto de IA chamado Aurora.",
        "Estamos fazendo parceria com a USP para a pesquisa."
    ]

    for texto in textos:
        print(f"\nğŸ“ Processando: {texto[:50]}...")
        memoria_entidades.extrair_e_atualizar_entidades(texto)

    print("\nğŸ“‹ Entidades Rastreadas:")
    for entidade, info in memoria_entidades.obter_todas_entidades().items():
        print(f"   - {entidade}: {info}")


def main():
    print("=" * 60)
    print("MEMORY/CONVERSATION (MEMÃ“RIA/CONVERSA) - Demo")
    print("=" * 60)

    token_tracker.reset()

    # Demo 1: MemÃ³ria Buffer
    demo_memoria_buffer()

    # Demo 2: MemÃ³ria Janela
    demo_memoria_janela()

    # Demo 3: MemÃ³ria de Entidades
    demo_memoria_entidades()

    print_total_usage(token_tracker, "TOTAL - Memory/Conversation")

    print("\n\n" + "=" * 60)
    print("Resumo dos Tipos de MemÃ³ria:")
    print("  - Buffer: HistÃ³rico completo, bom para conversas curtas")
    print("  - Janela: Ãšltimas N trocas, bom para conversas longas")
    print("  - Resumo: HistÃ³rico comprimido, previne estouro de tokens")
    print("  - Entidades: Rastreia entidades para persistÃªncia de conhecimento")
    print("=" * 60)

    print("\nFim do demo Memory/Conversation")
    print("=" * 60)


if __name__ == "__main__":
    main()
