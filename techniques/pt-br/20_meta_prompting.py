"""
Meta-Prompting (Meta-Prompting)

Tﾃｩcnica onde um LLM ﾃｩ usado para gerar, otimizar ou melhorar
prompts para outras tarefas de LLM. O LLM se torna um engenheiro de prompts.

Recursos:
- Geraﾃｧﾃ｣o automﾃ｡tica de prompts
- Otimizaﾃｧﾃ｣o e refinamento de prompts
- Criaﾃｧﾃ｣o de prompts especﾃｭficos para tarefas
- Teste A/B de prompts

Casos de uso:
- Engenharia de prompts automatizada
- Pipelines de otimizaﾃｧﾃ｣o de prompts
- Adaptaﾃｧﾃ｣o dinﾃ｢mica de prompts
- Geraﾃｧﾃ｣o de templates de prompts
"""

import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from langchain_core.prompts import ChatPromptTemplate
from config import (
    get_llm,
    TokenUsage,
    extract_tokens_from_response,
    print_token_usage,
    print_total_usage
)

# Rastreador global de tokens
token_tracker = TokenUsage()


def gerar_prompt(descricao_tarefa: str, contexto: str = "", restricoes: list[str] = None) -> str:
    """
    Gera um prompt otimizado para uma tarefa dada.

    Args:
        descricao_tarefa: O que o prompt deve realizar
        contexto: Contexto adicional sobre o caso de uso
        restricoes: Quaisquer restriﾃｧﾃｵes ou requisitos

    Retorna:
        Texto do prompt gerado
    """
    llm = get_llm(temperature=0.7)

    texto_restricoes = "\n".join([f"- {r}" for r in (restricoes or [])])

    prompt = ChatPromptTemplate.from_messages([
        ("system", """Vocﾃｪ ﾃｩ um engenheiro de prompts especialista. Sua tarefa ﾃｩ criar prompts altamente eficazes para LLMs.

Ao criar prompts, siga estas melhores prﾃ｡ticas:
1. Seja claro e especﾃｭfico sobre a tarefa
2. Inclua contexto relevante e exemplos se ﾃｺtil
3. Especifique o formato de saﾃｭda desejado
4. Adicione restriﾃｧﾃｵes ou proteﾃｧﾃｵes conforme necessﾃ｡rio
5. Use formataﾃｧﾃ｣o estruturada (cabeﾃｧalhos, marcadores) para clareza
6. Inclua uma persona/funﾃｧﾃ｣o se apropriado

Crie prompts que sejam:
- Inequﾃｭvocos e bem estruturados
- Focados na tarefa especﾃｭfica
- Projetados para obter respostas de alta qualidade"""),
        ("user", """Crie um prompt otimizado para a seguinte tarefa:

Descriﾃｧﾃ｣o da Tarefa: {descricao_tarefa}

Contexto Adicional: {contexto}

Restriﾃｧﾃｵes:
{restricoes}

Gere o prompt completo:""")
    ])

    chain = prompt | llm
    response = chain.invoke({
        "descricao_tarefa": descricao_tarefa,
        "contexto": contexto or "Nenhum fornecido",
        "restricoes": texto_restricoes or "Nenhuma especificada"
    })

    input_tokens, output_tokens = extract_tokens_from_response(response)
    token_tracker.add(input_tokens, output_tokens)
    print_token_usage(input_tokens, output_tokens, "Gerar")

    return response.content


def otimizar_prompt(prompt_original: str, problemas: list[str] = None, objetivos: list[str] = None) -> str:
    """
    Otimiza um prompt existente baseado em problemas identificados ou objetivos.

    Args:
        prompt_original: O prompt para otimizar
        problemas: Problemas conhecidos com o prompt
        objetivos: Objetivos de otimizaﾃｧﾃ｣o

    Retorna:
        Prompt otimizado
    """
    llm = get_llm(temperature=0.5)

    texto_problemas = "\n".join([f"- {p}" for p in (problemas or [])])
    texto_objetivos = "\n".join([f"- {o}" for o in (objetivos or [])])

    prompt = ChatPromptTemplate.from_messages([
        ("system", """Vocﾃｪ ﾃｩ um especialista em otimizar prompts para LLMs.
Analise o prompt dado e melhore-o preservando seu propﾃｳsito central.

Considere:
- Clareza e especificidade
- Estrutura e formataﾃｧﾃ｣o
- Contexto ou restriﾃｧﾃｵes faltantes
- Ambiguidades potenciais
- Especificaﾃｧﾃ｣o do formato de saﾃｭda"""),
        ("user", """Prompt Original:
{prompt_original}

Problemas Conhecidos:
{problemas}

Objetivos de Otimizaﾃｧﾃ｣o:
{objetivos}

Forneﾃｧa o prompt otimizado:""")
    ])

    chain = prompt | llm
    response = chain.invoke({
        "prompt_original": prompt_original,
        "problemas": texto_problemas or "Nenhum identificado",
        "objetivos": texto_objetivos or "Melhoria geral"
    })

    input_tokens, output_tokens = extract_tokens_from_response(response)
    token_tracker.add(input_tokens, output_tokens)
    print_token_usage(input_tokens, output_tokens, "Otimizar")

    return response.content


def avaliar_prompt(prompt: str, descricao_tarefa: str) -> dict:
    """
    Avalia a qualidade de um prompt e fornece feedback.

    Args:
        prompt: O prompt para avaliar
        descricao_tarefa: O que o prompt deve realizar

    Retorna:
        Dicionﾃ｡rio com pontuaﾃｧﾃｵes e feedback
    """
    llm = get_llm(temperature=0.3)

    prompt_avaliacao = ChatPromptTemplate.from_messages([
        ("system", """Vocﾃｪ ﾃｩ um especialista em avaliaﾃｧﾃ｣o de prompts. Analise prompts e forneﾃｧa feedback detalhado.

Avalie cada aspecto de 1-10 e explique seu raciocﾃｭnio:
1. Clareza: Quﾃ｣o claro e inequﾃｭvoco ﾃｩ o prompt?
2. Especificidade: Quﾃ｣o bem define a tarefa?
3. Estrutura: Estﾃ｡ bem organizado?
4. Completude: Inclui contexto necessﾃ｡rio?
5. Orientaﾃｧﾃ｣o de Saﾃｭda: Especifica o formato desejado?

Forneﾃｧa sugestﾃｵes de melhoria acionﾃ｡veis."""),
        ("user", """Tarefa que o prompt deve realizar:
{descricao_tarefa}

Prompt para avaliar:
{prompt}

Forneﾃｧa sua avaliaﾃｧﾃ｣o neste formato:
CLAREZA: [pontuaﾃｧﾃ｣o]/10 - [explicaﾃｧﾃ｣o]
ESPECIFICIDADE: [pontuaﾃｧﾃ｣o]/10 - [explicaﾃｧﾃ｣o]
ESTRUTURA: [pontuaﾃｧﾃ｣o]/10 - [explicaﾃｧﾃ｣o]
COMPLETUDE: [pontuaﾃｧﾃ｣o]/10 - [explicaﾃｧﾃ｣o]
ORIENTAﾃﾃグ DE SAﾃ好A: [pontuaﾃｧﾃ｣o]/10 - [explicaﾃｧﾃ｣o]

PONTUAﾃﾃグ GERAL: [mﾃｩdia]/10

SUGESTﾃ髭S DE MELHORIA:
[lista de sugestﾃｵes especﾃｭficas]""")
    ])

    chain = prompt_avaliacao | llm
    response = chain.invoke({
        "prompt": prompt,
        "descricao_tarefa": descricao_tarefa
    })

    input_tokens, output_tokens = extract_tokens_from_response(response)
    token_tracker.add(input_tokens, output_tokens)
    print_token_usage(input_tokens, output_tokens, "Avaliar")

    return {"avaliacao": response.content}


def gerar_variacoes_prompt(prompt_base: str, num_variacoes: int = 3) -> list[str]:
    """
    Gera variaﾃｧﾃｵes de um prompt para teste A/B.

    Args:
        prompt_base: O prompt base para variar
        num_variacoes: Nﾃｺmero de variaﾃｧﾃｵes para gerar

    Retorna:
        Lista de variaﾃｧﾃｵes de prompt
    """
    llm = get_llm(temperature=0.8)

    prompt = ChatPromptTemplate.from_messages([
        ("system", """Vocﾃｪ ﾃｩ um engenheiro de prompts criativo. Gere variaﾃｧﾃｵes distintas
de um prompt dado mantendo seu propﾃｳsito central.

Cada variaﾃｧﾃ｣o deve:
- Ter uma abordagem diferente para a tarefa
- Usar fraseado ou estrutura diferentes
- Potencialmente usar tﾃｩcnicas diferentes (few-shot, cadeia de pensamento, etc.)
- Ser claramente distinta das outras variaﾃｧﾃｵes"""),
        ("user", """Prompt Base:
{prompt_base}

Gere {num_variacoes} variaﾃｧﾃｵes distintas deste prompt.
Separe cada variaﾃｧﾃ｣o com "---VARIAﾃﾃグ---"

Variaﾃｧﾃｵes:""")
    ])

    chain = prompt | llm
    response = chain.invoke({
        "prompt_base": prompt_base,
        "num_variacoes": num_variacoes
    })

    input_tokens, output_tokens = extract_tokens_from_response(response)
    token_tracker.add(input_tokens, output_tokens)
    print_token_usage(input_tokens, output_tokens, "Variaﾃｧﾃｵes")

    # Analisa variaﾃｧﾃｵes
    conteudo = response.content
    variacoes = [v.strip() for v in conteudo.split("---VARIAﾃﾃグ---") if v.strip()]

    return variacoes[:num_variacoes]


def criar_template_prompt(tipo_tarefa: str, variaveis: list[str]) -> str:
    """
    Cria um template de prompt reutilizﾃ｡vel com variﾃ｡veis.

    Args:
        tipo_tarefa: Tipo de tarefa (sumarizaﾃｧﾃ｣o, classificaﾃｧﾃ｣o, etc.)
        variaveis: Lista de nomes de variﾃ｡veis para incluir

    Retorna:
        Template de prompt com placeholders {variavel}
    """
    llm = get_llm(temperature=0.5)

    texto_variaveis = ", ".join([f"{{{v}}}" for v in variaveis])

    prompt = ChatPromptTemplate.from_messages([
        ("system", """Vocﾃｪ ﾃｩ um designer de templates de prompts. Crie templates de prompts reutilizﾃ｡veis
com placeholders de variﾃ｡veis.

Use sintaxe {nome_variavel} para placeholders.
O template deve ser:
- Flexﾃｭvel o suficiente para lidar com diferentes entradas
- Especﾃｭfico o suficiente para produzir saﾃｭdas consistentes
- Bem estruturado e claro"""),
        ("user", """Crie um template de prompt para o seguinte:

Tipo de Tarefa: {tipo_tarefa}
Variﾃ｡veis Necessﾃ｡rias: {variaveis}

O template deve incluir estas variﾃ｡veis como placeholders: {texto_variaveis}

Template de Prompt:""")
    ])

    chain = prompt | llm
    response = chain.invoke({
        "tipo_tarefa": tipo_tarefa,
        "variaveis": ", ".join(variaveis),
        "texto_variaveis": texto_variaveis
    })

    input_tokens, output_tokens = extract_tokens_from_response(response)
    token_tracker.add(input_tokens, output_tokens)
    print_token_usage(input_tokens, output_tokens, "Template")

    return response.content


def auto_melhorar_prompt(prompt: str, tarefa: str, entrada_teste: str, max_iteracoes: int = 3) -> dict:
    """
    Melhora automaticamente um prompt atravﾃｩs de testes iterativos e refinamento.

    Args:
        prompt: Prompt inicial
        tarefa: Descriﾃｧﾃ｣o da tarefa
        entrada_teste: Entrada de exemplo para testar
        max_iteracoes: Mﾃ｡ximo de iteraﾃｧﾃｵes de melhoria

    Retorna:
        Dicionﾃ｡rio com prompt final e histﾃｳrico de melhorias
    """
    llm = get_llm(temperature=0.3)
    llm_tarefa = get_llm(temperature=0.7)

    historico = []
    prompt_atual = prompt

    for i in range(max_iteracoes):
        print(f"\n   Iteraﾃｧﾃ｣o {i+1}...")

        # Testa o prompt atual
        prompt_teste = ChatPromptTemplate.from_messages([
            ("system", prompt_atual),
            ("user", "{entrada}")
        ])

        chain_teste = prompt_teste | llm_tarefa
        resposta_teste = chain_teste.invoke({"entrada": entrada_teste})

        input_tokens, output_tokens = extract_tokens_from_response(resposta_teste)
        token_tracker.add(input_tokens, output_tokens)

        # Avalia e melhora
        prompt_melhoria = ChatPromptTemplate.from_messages([
            ("system", """Vocﾃｪ ﾃｩ um especialista em melhoria de prompts. Analise como um prompt performou
e sugira melhorias.

Avalie:
1. A saﾃｭda correspondeu ﾃ tarefa esperada?
2. A resposta foi bem estruturada?
3. Houve algum problema ou lacuna?

Forneﾃｧa uma versﾃ｣o melhorada do prompt."""),
            ("user", """Tarefa: {tarefa}

Prompt Atual:
{prompt_atual}

Entrada de Teste: {entrada_teste}

Saﾃｭda Gerada:
{saida}

Anﾃ｡lise e Prompt Melhorado:""")
        ])

        chain_melhoria = prompt_melhoria | llm
        resposta_melhoria = chain_melhoria.invoke({
            "tarefa": tarefa,
            "prompt_atual": prompt_atual,
            "entrada_teste": entrada_teste,
            "saida": resposta_teste.content
        })

        input_tokens, output_tokens = extract_tokens_from_response(resposta_melhoria)
        token_tracker.add(input_tokens, output_tokens)

        historico.append({
            "iteracao": i + 1,
            "prompt": prompt_atual,
            "saida": resposta_teste.content[:200] + "...",
            "feedback": resposta_melhoria.content[:200] + "..."
        })

        prompt_atual = resposta_melhoria.content

    return {
        "prompt_inicial": prompt,
        "prompt_final": prompt_atual,
        "iteracoes": len(historico),
        "historico": historico
    }


def main():
    print("=" * 60)
    print("META-PROMPTING - Demo")
    print("=" * 60)

    token_tracker.reset()

    # Exemplo 1: Gerar um Prompt
    print("\n統 GERAﾃﾃグ DE PROMPT")
    print("-" * 40)

    tarefa = "Extrair informaﾃｧﾃｵes chave de emails de suporte ao cliente e categorizﾃ｡-los"
    contexto = "Para uma empresa SaaS, emails podem conter relatﾃｳrios de bugs, pedidos de recursos, dﾃｺvidas de cobranﾃｧa"
    restricoes = ["Saﾃｭda deve ser formato JSON", "Incluir nﾃｭvel de urgﾃｪncia", "Suportar mﾃｺltiplos idiomas"]

    print(f"\nTarefa: {tarefa}")
    gerado = gerar_prompt(tarefa, contexto, restricoes)
    print(f"\n搭 Prompt Gerado:\n{gerado}")

    # Exemplo 2: Otimizar um Prompt
    print("\n\n肌 OTIMIZAﾃﾃグ DE PROMPT")
    print("-" * 40)

    original = "Resuma este artigo."
    problemas = ["Muito vago", "Sem orientaﾃｧﾃ｣o de tamanho", "Sem formato especificado"]
    objetivos = ["Tornar especﾃｭfico", "Adicionar formato de saﾃｭda", "Incluir extraﾃｧﾃ｣o de pontos chave"]

    print(f"\nOriginal: {original}")
    otimizado = otimizar_prompt(original, problemas, objetivos)
    print(f"\n搭 Prompt Otimizado:\n{otimizado}")

    # Exemplo 3: Avaliar um Prompt
    print("\n\n投 AVALIAﾃﾃグ DE PROMPT")
    print("-" * 40)

    prompt_para_avaliar = """Vocﾃｪ ﾃｩ um assistente ﾃｺtil. Responda a pergunta do usuﾃ｡rio."""
    desc_tarefa = "Criar um chatbot de suporte ao cliente que lida com problemas tﾃｩcnicos"

    print(f"\nPrompt: {prompt_para_avaliar}")
    print(f"Tarefa: {desc_tarefa}")
    avaliacao = avaliar_prompt(prompt_para_avaliar, desc_tarefa)
    print(f"\n搭 Avaliaﾃｧﾃ｣o:\n{avaliacao['avaliacao']}")

    # Exemplo 4: Gerar Variaﾃｧﾃｵes
    print("\n\n楳 VARIAﾃﾃ髭S DE PROMPT")
    print("-" * 40)

    base = "Explique {conceito} para um {publico}."
    print(f"\nBase: {base}")
    variacoes = gerar_variacoes_prompt(base, num_variacoes=3)
    print(f"\n搭 Variaﾃｧﾃｵes:")
    for i, v in enumerate(variacoes, 1):
        print(f"\n--- Variaﾃｧﾃ｣o {i} ---")
        print(v[:300] + "..." if len(v) > 300 else v)

    # Exemplo 5: Criar Template
    print("\n\n塘 CRIAﾃﾃグ DE TEMPLATE")
    print("-" * 40)

    tipo_tarefa = "anﾃ｡lise de sentimento"
    variaveis = ["texto", "idioma", "formato_saida"]

    print(f"\nTipo de Tarefa: {tipo_tarefa}")
    print(f"Variﾃ｡veis: {variaveis}")
    template = criar_template_prompt(tipo_tarefa, variaveis)
    print(f"\n搭 Template:\n{template}")

    print_total_usage(token_tracker, "TOTAL - Meta-Prompting")

    print("\n\n" + "=" * 60)
    print("Meta-Prompting permite que LLMs engenheirem prompts automaticamente,")
    print("habilitando otimizaﾃｧﾃ｣o e adaptaﾃｧﾃ｣o automatizada de prompts.")
    print("=" * 60)

    print("\nFim do demo Meta-Prompting")
    print("=" * 60)


if __name__ == "__main__":
    main()
