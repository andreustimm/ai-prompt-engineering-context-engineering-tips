"""
Ollama B√°sico - LLM Local

Usando modelos LLM locais via Ollama para aplica√ß√µes de IA
focadas em privacidade e offline.

Modelos dispon√≠veis:
- llama3.2: Prop√≥sito geral (padr√£o)
- mistral: R√°pido e eficiente
- codellama: Gera√ß√£o de c√≥digo
- phi3: Pequeno mas capaz

Pr√©-requisitos:
1. Instalar Ollama: https://ollama.ai
2. Iniciar servidor: ollama serve
3. Baixar modelo: ollama pull llama3.2

Casos de uso:
- Aplica√ß√µes sens√≠veis √† privacidade
- Opera√ß√£o offline
- Desenvolvimento/teste econ√¥mico
- Experimenta√ß√£o local
"""

import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from langchain_core.prompts import ChatPromptTemplate
from config import (
    get_ollama_llm,
    is_ollama_available,
    TokenUsage,
    print_total_usage
)

# Rastreador global de tokens (Ollama n√£o fornece contagem de tokens)
token_tracker = TokenUsage()


def verificar_status_ollama():
    """Verifica se o Ollama est√° rodando e dispon√≠vel."""
    if is_ollama_available():
        print("   ‚úì Ollama est√° rodando e acess√≠vel")
        return True
    else:
        print("   ‚úó Ollama n√£o est√° dispon√≠vel")
        print("   Por favor, certifique-se que o Ollama est√° instalado e rodando:")
        print("   1. Instale de https://ollama.ai")
        print("   2. Execute: ollama serve")
        print("   3. Baixe um modelo: ollama pull llama3.2")
        return False


def completar_basico(prompt: str, modelo: str = None) -> str:
    """Completar texto simples com Ollama."""
    llm = get_ollama_llm(model=modelo, temperature=0.7)

    response = llm.invoke(prompt)
    return response.content


def chat_com_prompt_sistema(mensagem_usuario: str, prompt_sistema: str, modelo: str = None) -> str:
    """Completar chat com prompt de sistema."""
    llm = get_ollama_llm(model=modelo, temperature=0.7)

    prompt = ChatPromptTemplate.from_messages([
        ("system", prompt_sistema),
        ("user", "{mensagem}")
    ])

    chain = prompt | llm
    response = chain.invoke({"mensagem": mensagem_usuario})
    return response.content


def resumir_texto(texto: str, modelo: str = None) -> str:
    """Resume texto usando LLM local."""
    llm = get_ollama_llm(model=modelo, temperature=0.3)

    prompt = ChatPromptTemplate.from_messages([
        ("system", "Voc√™ √© um especialista em resumos. Forne√ßa resumos concisos."),
        ("user", "Resuma o seguinte texto em 2-3 frases:\n\n{texto}")
    ])

    chain = prompt | llm
    response = chain.invoke({"texto": texto})
    return response.content


def traduzir_texto(texto: str, idioma_destino: str, modelo: str = None) -> str:
    """Traduz texto usando LLM local."""
    llm = get_ollama_llm(model=modelo, temperature=0.3)

    prompt = ChatPromptTemplate.from_messages([
        ("system", "Voc√™ √© um tradutor profissional. Traduza com precis√£o mantendo o significado original."),
        ("user", "Traduza o seguinte texto para {idioma}:\n\n{texto}")
    ])

    chain = prompt | llm
    response = chain.invoke({"texto": texto, "idioma": idioma_destino})
    return response.content


def gerar_codigo(descricao: str, linguagem: str = "Python", modelo: str = None) -> str:
    """Gera c√≥digo a partir de descri√ß√£o usando LLM local."""
    if modelo is None:
        modelo = "codellama"

    try:
        llm = get_ollama_llm(model=modelo, temperature=0.2)
    except Exception:
        llm = get_ollama_llm(temperature=0.2)

    prompt = ChatPromptTemplate.from_messages([
        ("system", f"Voc√™ √© um programador especialista em {linguagem}. Escreva c√≥digo limpo e bem documentado."),
        ("user", """Escreva c√≥digo {linguagem} para o seguinte:

{descricao}

Forne√ßa apenas o c√≥digo com coment√°rios, sem explica√ß√µes fora do c√≥digo.""")
    ])

    chain = prompt | llm
    response = chain.invoke({"descricao": descricao, "linguagem": linguagem})
    return response.content


def analisar_sentimento(texto: str, modelo: str = None) -> str:
    """Analisa sentimento do texto."""
    llm = get_ollama_llm(model=modelo, temperature=0)

    prompt = ChatPromptTemplate.from_messages([
        ("system", """Analise o sentimento do texto e responda com:
SENTIMENTO: [POSITIVO/NEGATIVO/NEUTRO]
CONFIAN√áA: [ALTA/M√âDIA/BAIXA]
EXPLICA√á√ÉO: [Breve explica√ß√£o]"""),
        ("user", "{texto}")
    ])

    chain = prompt | llm
    response = chain.invoke({"texto": texto})
    return response.content


def responder_pergunta(pergunta: str, contexto: str = None, modelo: str = None) -> str:
    """Responde uma pergunta, opcionalmente com contexto."""
    llm = get_ollama_llm(model=modelo, temperature=0.5)

    if contexto:
        prompt = ChatPromptTemplate.from_messages([
            ("system", "Responda perguntas baseado no contexto fornecido. Se a resposta n√£o estiver no contexto, diga isso."),
            ("user", """Contexto:
{contexto}

Pergunta: {pergunta}

Resposta:""")
        ])
        inputs = {"contexto": contexto, "pergunta": pergunta}
    else:
        prompt = ChatPromptTemplate.from_messages([
            ("system", "Voc√™ √© um assistente √∫til. Forne√ßa respostas precisas e informativas."),
            ("user", "{pergunta}")
        ])
        inputs = {"pergunta": pergunta}

    chain = prompt | llm
    response = chain.invoke(inputs)
    return response.content


def comparar_modelos(prompt: str, modelos: list[str] = None) -> dict:
    """Compara respostas de diferentes modelos."""
    if modelos is None:
        modelos = ["llama3.2", "mistral"]

    resultados = {}

    for modelo in modelos:
        try:
            print(f"\n   Testando {modelo}...")
            llm = get_ollama_llm(model=modelo, temperature=0.7)
            response = llm.invoke(prompt)
            resultados[modelo] = {
                "sucesso": True,
                "resposta": response.content
            }
        except Exception as e:
            resultados[modelo] = {
                "sucesso": False,
                "erro": str(e)
            }

    return resultados


def main():
    print("=" * 60)
    print("OLLAMA B√ÅSICO - Demo de LLM Local")
    print("=" * 60)

    # Verifica disponibilidade do Ollama
    print("\nüîç VERIFICANDO STATUS DO OLLAMA")
    print("-" * 40)

    if not verificar_status_ollama():
        print("\nDemo n√£o pode continuar sem o Ollama rodando.")
        print("Por favor, inicie o Ollama e tente novamente.")
        return

    # Reseta rastreador
    token_tracker.reset()

    # Exemplo 1: Completar B√°sico
    print("\n\nüìù COMPLETAR B√ÅSICO")
    print("-" * 40)

    prompt = "Explique o que √© intelig√™ncia artificial em um par√°grafo."
    print(f"\nPrompt: {prompt}")
    print("\nResposta:")
    print(completar_basico(prompt))

    # Exemplo 2: Chat com Prompt de Sistema
    print("\n\nüí¨ CHAT COM PROMPT DE SISTEMA")
    print("-" * 40)

    sistema = "Voc√™ √© um assistente de culin√°ria √∫til. Forne√ßa dicas pr√°ticas de cozinha."
    mensagem = "Como fazer ovos mexidos fofos?"

    print(f"\nSistema: {sistema}")
    print(f"Usu√°rio: {mensagem}")
    print("\nResposta:")
    print(chat_com_prompt_sistema(mensagem, sistema))

    # Exemplo 3: Sumariza√ß√£o
    print("\n\nüìã SUMARIZA√á√ÉO DE TEXTO")
    print("-" * 40)

    texto_longo = """
    Machine learning √© um subconjunto da intelig√™ncia artificial que permite que computadores
    aprendam e melhorem a partir da experi√™ncia sem serem explicitamente programados.
    Foca no desenvolvimento de algoritmos que podem acessar dados, aprender com eles,
    e fazer previs√µes ou decis√µes. Algoritmos de machine learning s√£o usados em
    uma grande variedade de aplica√ß√µes, como filtragem de email, vis√£o computacional,
    e reconhecimento de fala. Os tr√™s tipos principais s√£o aprendizado supervisionado,
    aprendizado n√£o supervisionado e aprendizado por refor√ßo, cada um adequado para diferentes
    tipos de problemas e dados.
    """

    print(f"\nOriginal ({len(texto_longo)} caracteres)")
    print("\nResumo:")
    print(resumir_texto(texto_longo))

    # Exemplo 4: Tradu√ß√£o
    print("\n\nüåê TRADU√á√ÉO")
    print("-" * 40)

    texto_para_traduzir = "The quick brown fox jumps over the lazy dog."
    print(f"\nOriginal: {texto_para_traduzir}")
    print("\nPortugu√™s:")
    print(traduzir_texto(texto_para_traduzir, "Portugu√™s"))

    # Exemplo 5: An√°lise de Sentimento
    print("\n\nüòä AN√ÅLISE DE SENTIMENTO")
    print("-" * 40)

    avaliacoes = [
        "Este produto superou todas as minhas expectativas! Amei absolutamente!",
        "Experi√™ncia terr√≠vel. O item chegou quebrado e o suporte n√£o ajudou.",
        "√â ok. Faz o que deveria fazer, nada especial."
    ]

    for avaliacao in avaliacoes:
        print(f"\nAvalia√ß√£o: {avaliacao[:50]}...")
        print(analisar_sentimento(avaliacao))

    # Exemplo 6: Gera√ß√£o de C√≥digo
    print("\n\nüíª GERA√á√ÉO DE C√ìDIGO")
    print("-" * 40)

    pedido_codigo = "uma fun√ß√£o que calcula a sequ√™ncia de Fibonacci at√© n n√∫meros"
    print(f"\nPedido: {pedido_codigo}")
    print("\nC√≥digo Gerado:")
    print(gerar_codigo(pedido_codigo, "Python"))

    # Exemplo 7: Q&A
    print("\n\n‚ùì PERGUNTAS E RESPOSTAS")
    print("-" * 40)

    contexto = """
    A Torre Eiffel √© uma torre de treli√ßa de ferro forjado no Champ de Mars em Paris.
    Foi constru√≠da de 1887 a 1889 como pe√ßa central da Feira Mundial de 1889.
    A torre tem 330 metros de altura e foi a estrutura feita pelo homem mais alta do mundo
    at√© 1930. Recebeu o nome do engenheiro Gustave Eiffel, cuja empresa projetou
    e construiu a torre.
    """

    pergunta = "Quando a Torre Eiffel foi constru√≠da e qual sua altura?"
    print(f"\nContexto fornecido: Sim")
    print(f"Pergunta: {pergunta}")
    print("\nResposta:")
    print(responder_pergunta(pergunta, contexto))

    print("\n\n" + "=" * 60)
    print("Nota: Ollama n√£o fornece contagem de tokens como OpenAI.")
    print("Para rastreamento de custos, monitore logs do servidor Ollama ou use timing.")
    print("=" * 60)

    print("\nFim do demo Ollama B√°sico")
    print("=" * 60)


if __name__ == "__main__":
    main()
