"""
Estrat√©gias Avan√ßadas de Chunking

Explora diferentes estrat√©gias de divis√£o de texto para sistemas RAG.
A qualidade do chunking impacta significativamente a efic√°cia da recupera√ß√£o.

Estrat√©gias implementadas:
1. RecursiveCharacterTextSplitter - Divis√£o hier√°rquica por caracteres
2. SentenceTransformers Token Splitter - Divis√£o por tokens
3. Semantic Chunker - Divis√£o por similaridade sem√¢ntica
4. Markdown Splitter - Divis√£o consciente de estrutura markdown
5. Janela deslizante personalizada - Chunks sobrepostos configur√°veis

Casos de uso:
- Otimiza√ß√£o da qualidade de recupera√ß√£o RAG
- Manipula√ß√£o de diferentes tipos de documentos
- Equil√≠brio entre tamanho do chunk e preserva√ß√£o de contexto
"""

import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from langchain.text_splitter import (
    RecursiveCharacterTextSplitter,
    CharacterTextSplitter,
    MarkdownTextSplitter,
    TokenTextSplitter
)
from langchain_core.documents import Document

from config import (
    get_llm,
    get_embeddings,
    TokenUsage,
    extract_tokens_from_response,
    print_token_usage,
    print_total_usage
)

# Rastreador global de tokens
token_tracker = TokenUsage()


def carregar_documento_exemplo() -> str:
    """Carrega documento de exemplo para demonstra√ß√µes de chunking."""
    sample_path = Path(__file__).parent.parent.parent / "sample_data" / "documents" / "long_document.txt"

    if sample_path.exists():
        with open(sample_path, 'r', encoding='utf-8') as f:
            return f.read()
    else:
        # Texto de exemplo alternativo
        return """
        # Introdu√ß√£o ao Machine Learning

        Machine learning √© um subconjunto de intelig√™ncia artificial que permite que sistemas aprendam e melhorem com a experi√™ncia sem serem explicitamente programados.

        ## Tipos de Machine Learning

        ### Aprendizado Supervisionado
        O aprendizado supervisionado usa dados rotulados para treinar modelos. O algoritmo aprende com pares de entrada-sa√≠da.
        Algoritmos comuns incluem:
        - Regress√£o Linear
        - √Årvores de Decis√£o
        - M√°quinas de Vetores de Suporte
        - Redes Neurais

        ### Aprendizado N√£o Supervisionado
        O aprendizado n√£o supervisionado encontra padr√µes em dados n√£o rotulados. Ele descobre estruturas ocultas.
        Algoritmos comuns incluem:
        - Clustering K-Means
        - Clustering Hier√°rquico
        - An√°lise de Componentes Principais

        ### Aprendizado por Refor√ßo
        O aprendizado por refor√ßo treina agentes atrav√©s de recompensas e penalidades.
        Aplica√ß√µes incluem:
        - Jogos (AlphaGo)
        - Rob√≥tica
        - Ve√≠culos aut√¥nomos

        ## Deep Learning

        Deep learning usa redes neurais com m√∫ltiplas camadas para modelar padr√µes complexos.
        Arquiteturas principais:
        - Redes Neurais Convolucionais (CNNs) para imagens
        - Redes Neurais Recorrentes (RNNs) para sequ√™ncias
        - Transformers para compreens√£o de linguagem

        ## Melhores Pr√°ticas

        1. Comece com dados limpos e de qualidade
        2. Escolha algoritmos apropriados para seu problema
        3. Use valida√ß√£o cruzada para avalia√ß√£o de modelos
        4. Monitore overfitting e underfitting
        5. Documente seus experimentos e resultados
        """


def chunking_caracteres_recursivo(texto: str, tamanho_chunk: int = 500, sobreposicao: int = 100) -> list[Document]:
    """
    Divide texto usando divis√£o recursiva de caracteres com hierarquia.

    Este splitter tenta dividir nestes separadores em ordem:
    ["\n\n", "\n", " ", ""]

    Ele divide recursivamente em separadores menores quando chunks s√£o muito grandes.
    """
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=tamanho_chunk,
        chunk_overlap=sobreposicao,
        length_function=len,
        separators=["\n\n", "\n", ". ", " ", ""]
    )

    chunks = splitter.create_documents([texto])

    for i, chunk in enumerate(chunks):
        chunk.metadata["indice_chunk"] = i
        chunk.metadata["estrategia"] = "caracteres_recursivo"
        chunk.metadata["tamanho_chunk"] = len(chunk.page_content)

    return chunks


def chunking_por_tokens(texto: str, tamanho_chunk: int = 200, sobreposicao: int = 50) -> list[Document]:
    """
    Divide texto baseado em contagem de tokens em vez de caracteres.

    Isso garante que chunks caibam nos limites de contexto do modelo.
    Divis√£o por tokens √© mais precisa para processamento LLM.
    """
    splitter = TokenTextSplitter(
        chunk_size=tamanho_chunk,
        chunk_overlap=sobreposicao
    )

    chunks = splitter.create_documents([texto])

    for i, chunk in enumerate(chunks):
        chunk.metadata["indice_chunk"] = i
        chunk.metadata["estrategia"] = "por_tokens"
        chunk.metadata["tamanho_chunk"] = len(chunk.page_content)

    return chunks


def chunking_markdown(texto: str, tamanho_chunk: int = 500, sobreposicao: int = 100) -> list[Document]:
    """
    Divide texto markdown respeitando sua estrutura.

    Este splitter entende cabe√ßalhos e blocos de c√≥digo markdown,
    preservando a estrutura do documento nos chunks.
    """
    splitter = MarkdownTextSplitter(
        chunk_size=tamanho_chunk,
        chunk_overlap=sobreposicao
    )

    chunks = splitter.create_documents([texto])

    for i, chunk in enumerate(chunks):
        chunk.metadata["indice_chunk"] = i
        chunk.metadata["estrategia"] = "markdown"
        chunk.metadata["tamanho_chunk"] = len(chunk.page_content)

    return chunks


def chunking_semantico(texto: str, limiar_quebra: float = 0.5) -> list[Document]:
    """
    Divide texto baseado em similaridade sem√¢ntica entre senten√ßas.

    Agrupa senten√ßas semanticamente similares,
    criando chunks mais coerentes para recupera√ß√£o.

    Nota: Esta √© uma implementa√ß√£o simplificada. Para produ√ß√£o,
    use langchain_experimental.text_splitter.SemanticChunker
    """
    try:
        from langchain_experimental.text_splitter import SemanticChunker

        embeddings = get_embeddings()
        splitter = SemanticChunker(
            embeddings=embeddings,
            breakpoint_threshold_type="percentile",
            breakpoint_threshold_amount=limiar_quebra * 100
        )

        chunks = splitter.create_documents([texto])

        for i, chunk in enumerate(chunks):
            chunk.metadata["indice_chunk"] = i
            chunk.metadata["estrategia"] = "semantico"
            chunk.metadata["tamanho_chunk"] = len(chunk.page_content)

        return chunks

    except ImportError:
        print("   Aviso: langchain_experimental n√£o dispon√≠vel. Usando fallback.")
        return chunking_caracteres_recursivo(texto)


def chunking_janela_deslizante(texto: str, tamanho_janela: int = 500, passo: int = 250) -> list[Document]:
    """
    Cria chunks sobrepostos usando abordagem de janela deslizante.

    Args:
        texto: Texto de entrada para dividir
        tamanho_janela: Tamanho de cada chunk em caracteres
        passo: Quanto avan√ßar a janela (tamanho_janela - sobreposi√ß√£o)

    Isso cria m√°xima sobreposi√ß√£o entre chunks, √∫til quando
    continuidade de contexto √© cr√≠tica.
    """
    chunks = []
    inicio = 0

    while inicio < len(texto):
        fim = inicio + tamanho_janela
        texto_chunk = texto[inicio:fim]

        # Evita cortar palavras no meio
        if fim < len(texto) and texto[fim] not in ' \n\t':
            # Encontra √∫ltimo espa√ßo dentro do chunk
            ultimo_espaco = texto_chunk.rfind(' ')
            if ultimo_espaco > 0:
                texto_chunk = texto_chunk[:ultimo_espaco]

        chunk = Document(
            page_content=texto_chunk.strip(),
            metadata={
                "indice_chunk": len(chunks),
                "estrategia": "janela_deslizante",
                "char_inicio": inicio,
                "tamanho_chunk": len(texto_chunk.strip())
            }
        )
        chunks.append(chunk)

        inicio += passo

    return chunks


def chunking_por_sentencas(texto: str, sentencas_por_chunk: int = 5, sobreposicao_sentencas: int = 1) -> list[Document]:
    """
    Divide texto em chunks contendo n√∫mero fixo de senten√ßas.

    Preserva limites de senten√ßa, garantindo que chunks sejam gramaticalmente completos.
    """
    import re

    # Divis√£o simples de senten√ßas
    sentencas = re.split(r'(?<=[.!?])\s+', texto)
    sentencas = [s.strip() for s in sentencas if s.strip()]

    chunks = []
    i = 0

    while i < len(sentencas):
        sentencas_chunk = sentencas[i:i + sentencas_por_chunk]
        texto_chunk = ' '.join(sentencas_chunk)

        chunk = Document(
            page_content=texto_chunk,
            metadata={
                "indice_chunk": len(chunks),
                "estrategia": "por_sentencas",
                "contagem_sentencas": len(sentencas_chunk),
                "tamanho_chunk": len(texto_chunk)
            }
        )
        chunks.append(chunk)

        i += sentencas_por_chunk - sobreposicao_sentencas

    return chunks


def comparar_estrategias_chunking(texto: str):
    """Compara todas as estrat√©gias de chunking no mesmo texto."""

    print("\n   Comparando estrat√©gias de chunking no texto de exemplo...")
    print(f"   Tamanho do texto original: {len(texto)} caracteres")

    estrategias = {
        "Caracteres Recursivo": chunking_caracteres_recursivo(texto, 500, 100),
        "Por Tokens": chunking_por_tokens(texto, 200, 50),
        "Markdown": chunking_markdown(texto, 500, 100),
        "Janela Deslizante": chunking_janela_deslizante(texto, 500, 250),
        "Por Senten√ßas": chunking_por_sentencas(texto, 5, 1)
    }

    print("\n   " + "=" * 50)
    print("   COMPARA√á√ÉO DE ESTRAT√âGIAS DE CHUNKING")
    print("   " + "=" * 50)

    for nome, chunks in estrategias.items():
        tamanhos = [len(c.page_content) for c in chunks]
        media = sum(tamanhos) / len(tamanhos) if tamanhos else 0

        print(f"\n   {nome}:")
        print(f"      N√∫mero de chunks: {len(chunks)}")
        print(f"      Tamanho m√©dio: {media:.1f} caracteres")
        print(f"      M√≠n/M√°x: {min(tamanhos)}/{max(tamanhos)} caracteres")

    return estrategias


def analisar_qualidade_chunks(chunks: list[Document]) -> dict:
    """
    Analisa a qualidade dos chunks usando o LLM.

    Avalia coer√™ncia, completude e densidade de informa√ß√£o.
    """
    llm = get_llm(temperature=0)

    # Amostra alguns chunks para an√°lise
    chunks_amostra = chunks[:3] if len(chunks) > 3 else chunks

    resultados = []

    for chunk in chunks_amostra:
        from langchain_core.prompts import ChatPromptTemplate

        prompt = ChatPromptTemplate.from_messages([
            ("system", """Voc√™ √© um especialista em avaliar qualidade de chunks de texto para sistemas RAG.
Analise o chunk fornecido e avalie-o nestes crit√©rios (1-10):
1. Coer√™ncia: Forma uma unidade completa e compreens√≠vel?
2. Densidade de Informa√ß√£o: Quanta informa√ß√£o √∫til cont√©m?
3. Independ√™ncia de Contexto: Pode ser entendido sem o texto ao redor?

Retorne um objeto JSON com pontua√ß√µes e explica√ß√µes breves."""),
            ("user", "Chunk para analisar:\n\n{chunk}")
        ])

        chain = prompt | llm
        response = chain.invoke({"chunk": chunk.page_content[:1000]})

        input_tokens, output_tokens = extract_tokens_from_response(response)
        token_tracker.add(input_tokens, output_tokens)

        resultados.append({
            "indice_chunk": chunk.metadata.get("indice_chunk"),
            "analise": response.content
        })

    return resultados


def demo_chunking_para_recuperacao():
    """Demonstra como diferentes chunkings afetam a recupera√ß√£o."""

    print("\n   Demonstrando impacto do chunking na recupera√ß√£o...")

    texto = carregar_documento_exemplo()

    # Cria chunks com diferentes estrat√©gias
    chunks_pequenos = chunking_caracteres_recursivo(texto, 300, 50)
    chunks_medios = chunking_caracteres_recursivo(texto, 800, 150)
    chunks_grandes = chunking_caracteres_recursivo(texto, 1500, 300)

    print("\n   Compara√ß√£o de tamanhos de chunk:")
    print(f"      Pequeno (300 chars): {len(chunks_pequenos)} chunks")
    print(f"      M√©dio (800 chars):   {len(chunks_medios)} chunks")
    print(f"      Grande (1500 chars): {len(chunks_grandes)} chunks")

    # Chunks de exemplo
    print("\n   Exemplo de chunk pequeno (primeiros 200 chars):")
    print(f"      '{chunks_pequenos[0].page_content[:200]}...'")

    print("\n   Exemplo de chunk grande (primeiros 200 chars):")
    print(f"      '{chunks_grandes[0].page_content[:200]}...'")

    print("\n   Trade-offs:")
    print("      - Chunks menores: Melhor precis√£o, mas pode perder contexto")
    print("      - Chunks maiores: Mais contexto, mas pode incluir info irrelevante")
    print("      - Sobreposi√ß√£o: Ajuda a preservar contexto entre chunks")


def main():
    print("=" * 60)
    print("ESTRAT√âGIAS AVAN√áADAS DE CHUNKING")
    print("=" * 60)

    token_tracker.reset()

    # Carrega documento de exemplo
    print("\nüìö CARREGANDO DOCUMENTO DE EXEMPLO")
    print("-" * 40)

    texto = carregar_documento_exemplo()
    print(f"   Documento carregado com {len(texto)} caracteres")

    # Demo 1: Comparar estrat√©gias
    print("\n\nüìä COMPARA√á√ÉO DE ESTRAT√âGIAS")
    print("-" * 40)

    estrategias = comparar_estrategias_chunking(texto)

    # Demo 2: Mostrar exemplos de chunks
    print("\n\nüìù EXEMPLOS DE CHUNKS")
    print("-" * 40)

    chunks_recursivo = estrategias["Caracteres Recursivo"]
    print(f"\n   Primeiros 3 chunks da estrat√©gia Caracteres Recursivo:\n")

    for chunk in chunks_recursivo[:3]:
        preview = chunk.page_content[:150].replace('\n', ' ')
        print(f"   Chunk {chunk.metadata['indice_chunk']}:")
        print(f"   Tamanho: {chunk.metadata['tamanho_chunk']} chars")
        print(f"   Preview: {preview}...")
        print()

    # Demo 3: Analisar qualidade dos chunks
    print("\n\nüîç AN√ÅLISE DE QUALIDADE DOS CHUNKS")
    print("-" * 40)

    print("\n   Analisando qualidade dos chunks usando LLM...")
    resultados_qualidade = analisar_qualidade_chunks(chunks_recursivo)

    for resultado in resultados_qualidade:
        print(f"\n   An√°lise do Chunk {resultado['indice_chunk']}:")
        print(f"   {resultado['analise'][:300]}...")

    # Demo 4: Impacto do chunking na recupera√ß√£o
    print("\n\nüìà IMPACTO DO CHUNKING NA RECUPERA√á√ÉO")
    print("-" * 40)

    demo_chunking_para_recuperacao()

    # Melhores pr√°ticas
    print("\n\nüí° MELHORES PR√ÅTICAS DE CHUNKING")
    print("-" * 40)
    print("""
   1. Escolha o tamanho do chunk baseado no tipo de conte√∫do:
      - Docs t√©cnicos: 500-1000 chars (preserva contexto)
      - FAQs: 200-400 chars (uma P&R por chunk)
      - Artigos: 800-1500 chars (n√≠vel de par√°grafo)

   2. Use sobreposi√ß√£o apropriada:
      - 10-20% do tamanho do chunk √© t√≠pico
      - Mais sobreposi√ß√£o para conte√∫do altamente conectado
      - Menos para se√ß√µes distintas e independentes

   3. Combine estrat√©gia com estrutura do documento:
      - Markdown/HTML: Use splitters conscientes de estrutura
      - Texto puro: Divis√£o recursiva ou sem√¢ntica
      - C√≥digo: Use splitters espec√≠ficos por linguagem

   4. Considere as necessidades de recupera√ß√£o:
      - Busca sem√¢ntica: Chunks maiores e coerentes
      - Busca por palavra-chave: Chunks menores e focados
      - H√≠brido: Chunks m√©dios com boa sobreposi√ß√£o
    """)

    print_total_usage(token_tracker, "TOTAL - Chunking Avan√ßado")

    print("\nFim da demonstra√ß√£o de Chunking Avan√ßado")
    print("=" * 60)


if __name__ == "__main__":
    main()
