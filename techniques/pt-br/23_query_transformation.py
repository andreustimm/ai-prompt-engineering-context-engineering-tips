"""
Transforma√ß√£o de Consultas

Transforma consultas do usu√°rio para melhorar a efic√°cia da recupera√ß√£o.
Diferentes estrat√©gias de transforma√ß√£o ajudam a preencher a lacuna entre
como usu√°rios formulam consultas e como documentos relevantes s√£o escritos.

T√©cnicas implementadas:
1. HyDE - Embeddings de Documentos Hipot√©ticos
2. Multi-Query - Gerar m√∫ltiplas varia√ß√µes da consulta
3. Step-Back - Abstrair para conceitos mais amplos
4. Decomposi√ß√£o - Quebrar consultas complexas em sub-consultas

Casos de uso:
- Melhorar recall de recupera√ß√£o para consultas vagas
- Lidar com perguntas complexas de m√∫ltiplas partes
- Resolver incompatibilidade de vocabul√°rio
"""

import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from langchain_core.prompts import ChatPromptTemplate
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_core.documents import Document

# Imports condicionais
try:
    from langchain_community.vectorstores import Chroma
    CHROMA_DISPONIVEL = True
except ImportError:
    CHROMA_DISPONIVEL = False
    print("Aviso: chromadb n√£o instalado. Execute: pip install chromadb")

from config import (
    get_llm,
    get_embeddings,
    TokenUsage,
    extract_tokens_from_response,
    print_token_usage,
    print_total_usage
)

# Rastreador global de tokens
token_tracker = TokenUsage()


def carregar_documentos_exemplo() -> list[Document]:
    """Carrega documentos de exemplo para demonstra√ß√£o."""
    sample_dir = Path(__file__).parent.parent.parent / "sample_data" / "documents"
    documentos = []

    for file_path in sample_dir.glob("*.txt"):
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                conteudo = f.read()
                documentos.append(Document(
                    page_content=conteudo,
                    metadata={"fonte": file_path.name}
                ))
        except Exception as e:
            print(f"   Aviso: N√£o foi poss√≠vel carregar {file_path}: {e}")

    return documentos


def criar_vectorstore(documentos: list[Document], nome_colecao: str = "transformacao_query"):
    """Cria vector store a partir de documentos."""
    if not CHROMA_DISPONIVEL:
        raise ImportError("chromadb √© necess√°rio")

    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=500,
        chunk_overlap=100
    )
    chunks = text_splitter.split_documents(documentos)

    embeddings = get_embeddings()
    vectorstore = Chroma.from_documents(
        documents=chunks,
        embedding=embeddings,
        collection_name=nome_colecao
    )

    return vectorstore


def transformar_hyde(consulta: str) -> str:
    """
    HyDE - Embeddings de Documentos Hipot√©ticos

    Em vez de fazer embedding da consulta diretamente, gera um documento
    hipot√©tico que responderia √† consulta, ent√£o usa isso para recupera√ß√£o.

    Isso frequentemente produz melhores correspond√™ncias porque o documento
    gerado √© mais similar em estilo e vocabul√°rio aos documentos reais.
    """
    llm = get_llm(temperature=0.7)

    prompt = ChatPromptTemplate.from_messages([
        ("system", """Voc√™ √© um escritor t√©cnico especialista. Dada uma pergunta, escreva um par√°grafo detalhado
que apareceria em um documento respondendo essa pergunta. Escreva como se este par√°grafo existisse em
um documento real, n√£o como uma resposta direta √† pergunta.

Seja espec√≠fico, factual e abrangente. N√£o comece com frases como "Aqui est√°" ou "Este par√°grafo"."""),
        ("user", "Pergunta: {consulta}\n\nEscreva um par√°grafo de documento que conteria a resposta:")
    ])

    chain = prompt | llm
    response = chain.invoke({"consulta": consulta})

    input_tokens, output_tokens = extract_tokens_from_response(response)
    token_tracker.add(input_tokens, output_tokens)
    print_token_usage(input_tokens, output_tokens, "Gera√ß√£o HyDE")

    return response.content


def transformar_multi_query(consulta: str, num_consultas: int = 3) -> list[str]:
    """
    Transforma√ß√£o Multi-Query

    Gera m√∫ltiplas varia√ß√µes da consulta original para melhorar
    o recall. Diferentes formula√ß√µes podem corresponder a diferentes documentos relevantes.
    """
    llm = get_llm(temperature=0.8)

    prompt = ChatPromptTemplate.from_messages([
        ("system", """Voc√™ √© especialista em reformular consultas de busca.
Dada uma pergunta do usu√°rio, gere {num_consultas} vers√µes diferentes da pergunta
que poderiam ser usadas para buscar informa√ß√µes relevantes.

Cada vers√£o deve:
- Capturar a mesma inten√ß√£o
- Usar vocabul√°rio/formula√ß√£o diferente
- Potencialmente enfatizar diferentes aspectos

Retorne APENAS as consultas, uma por linha, numeradas 1., 2., etc."""),
        ("user", "Pergunta original: {consulta}\n\nGere {num_consultas} varia√ß√µes da consulta:")
    ])

    chain = prompt | llm
    response = chain.invoke({"consulta": consulta, "num_consultas": num_consultas})

    input_tokens, output_tokens = extract_tokens_from_response(response)
    token_tracker.add(input_tokens, output_tokens)
    print_token_usage(input_tokens, output_tokens, "Gera√ß√£o Multi-Query")

    # Analisa a resposta
    linhas = response.content.strip().split('\n')
    consultas = []
    for linha in linhas:
        limpa = linha.strip()
        if limpa and limpa[0].isdigit():
            limpa = limpa.split('.', 1)[-1].strip()
        if limpa:
            consultas.append(limpa)

    return consultas[:num_consultas]


def transformar_step_back(consulta: str) -> str:
    """
    Step-Back Prompting

    Transforma uma pergunta espec√≠fica em uma pergunta mais geral/abstrata.
    Isso pode ajudar a recuperar informa√ß√µes fundamentais que fornecem
    contexto para responder a pergunta espec√≠fica.
    """
    llm = get_llm(temperature=0.3)

    prompt = ChatPromptTemplate.from_messages([
        ("system", """Voc√™ √© especialista em abstrair perguntas para seus conceitos subjacentes.
Dada uma pergunta espec√≠fica, gere uma pergunta "step-back" mais geral que aborde
o conceito ou princ√≠pio mais amplo por tr√°s da pergunta original.

A pergunta step-back deve:
- Ser mais geral/abstrata
- Cobrir o conhecimento fundamental necess√°rio
- Ajudar a recuperar informa√ß√µes de contexto

Retorne APENAS a pergunta step-back, nada mais."""),
        ("user", """Pergunta original: {consulta}

Pergunta step-back:""")
    ])

    chain = prompt | llm
    response = chain.invoke({"consulta": consulta})

    input_tokens, output_tokens = extract_tokens_from_response(response)
    token_tracker.add(input_tokens, output_tokens)
    print_token_usage(input_tokens, output_tokens, "Gera√ß√£o Step-Back")

    return response.content.strip()


def decompor_consulta(consulta: str) -> list[str]:
    """
    Decomposi√ß√£o de Consulta

    Quebra uma pergunta complexa em sub-perguntas mais simples.
    Cada sub-pergunta pode ser respondida independentemente, e as
    respostas combinadas para abordar a pergunta original.
    """
    llm = get_llm(temperature=0.3)

    prompt = ChatPromptTemplate.from_messages([
        ("system", """Voc√™ √© especialista em decompor perguntas complexas.
Dada uma pergunta complexa, decomponha-a em 2-4 sub-perguntas mais simples que,
quando respondidas juntas, forneceriam uma resposta completa √† pergunta original.

Cada sub-pergunta deve:
- Ser autocontida e respond√≠vel independentemente
- Abordar um aspecto espec√≠fico da pergunta original
- Contribuir para uma resposta abrangente

Retorne APENAS as sub-perguntas, uma por linha, numeradas 1., 2., etc."""),
        ("user", """Pergunta complexa: {consulta}

Sub-perguntas:""")
    ])

    chain = prompt | llm
    response = chain.invoke({"consulta": consulta})

    input_tokens, output_tokens = extract_tokens_from_response(response)
    token_tracker.add(input_tokens, output_tokens)
    print_token_usage(input_tokens, output_tokens, "Decomposi√ß√£o de Consulta")

    # Analisa a resposta
    linhas = response.content.strip().split('\n')
    consultas = []
    for linha in linhas:
        limpa = linha.strip()
        if limpa and limpa[0].isdigit():
            limpa = limpa.split('.', 1)[-1].strip()
        if limpa:
            consultas.append(limpa)

    return consultas


def recuperar_com_transformacao(
    vectorstore,
    consulta: str,
    tipo_transformacao: str = "hyde",
    k: int = 3
) -> list[Document]:
    """
    Recupera documentos usando uma consulta transformada.

    Args:
        vectorstore: Vector store para buscar
        consulta: Consulta original do usu√°rio
        tipo_transformacao: Tipo de transforma√ß√£o ("hyde", "multi_query", "step_back", "decompor")
        k: N√∫mero de documentos a recuperar
    """
    retriever = vectorstore.as_retriever(search_kwargs={"k": k})

    if tipo_transformacao == "hyde":
        # Gera documento hipot√©tico e busca com ele
        doc_hipotetico = transformar_hyde(consulta)
        print(f"\n   Documento HyDE (primeiros 200 chars): {doc_hipotetico[:200]}...")
        return retriever.invoke(doc_hipotetico)

    elif tipo_transformacao == "multi_query":
        # Busca com m√∫ltiplas varia√ß√µes e combina resultados
        consultas = transformar_multi_query(consulta)
        print(f"\n   Consultas geradas:")
        for q in consultas:
            print(f"      - {q}")

        todos_docs = []
        vistos = set()
        for q in consultas:
            docs = retriever.invoke(q)
            for doc in docs:
                chave_doc = doc.page_content[:100]
                if chave_doc not in vistos:
                    vistos.add(chave_doc)
                    todos_docs.append(doc)

        return todos_docs[:k * 2]

    elif tipo_transformacao == "step_back":
        # Busca com pergunta original e step-back
        step_back = transformar_step_back(consulta)
        print(f"\n   Pergunta step-back: {step_back}")

        docs_originais = retriever.invoke(consulta)
        docs_step_back = retriever.invoke(step_back)

        # Combina, priorizando original
        todos_docs = docs_originais.copy()
        vistos = {doc.page_content[:100] for doc in docs_originais}
        for doc in docs_step_back:
            if doc.page_content[:100] not in vistos:
                todos_docs.append(doc)

        return todos_docs[:k * 2]

    elif tipo_transformacao == "decompor":
        # Busca com sub-perguntas e combina resultados
        sub_consultas = decompor_consulta(consulta)
        print(f"\n   Sub-perguntas:")
        for q in sub_consultas:
            print(f"      - {q}")

        todos_docs = []
        vistos = set()
        for q in sub_consultas:
            docs = retriever.invoke(q)
            for doc in docs:
                chave_doc = doc.page_content[:100]
                if chave_doc not in vistos:
                    vistos.add(chave_doc)
                    todos_docs.append(doc)

        return todos_docs[:k * 2]

    else:
        # Sem transforma√ß√£o
        return retriever.invoke(consulta)


def gerar_resposta(consulta: str, documentos: list[Document]) -> str:
    """Gera resposta usando contexto recuperado."""
    llm = get_llm(temperature=0.3)

    contexto = "\n\n---\n\n".join([doc.page_content for doc in documentos])

    prompt = ChatPromptTemplate.from_messages([
        ("system", """Voc√™ √© um assistente √∫til. Responda a pergunta baseado no contexto fornecido.
Seja completo mas conciso. Se o contexto n√£o contiver informa√ß√£o suficiente, diga isso."""),
        ("user", """Contexto:
{contexto}

Pergunta: {pergunta}

Resposta:""")
    ])

    chain = prompt | llm
    response = chain.invoke({"contexto": contexto, "pergunta": consulta})

    input_tokens, output_tokens = extract_tokens_from_response(response)
    token_tracker.add(input_tokens, output_tokens)
    print_token_usage(input_tokens, output_tokens, "Gera√ß√£o de Resposta")

    return response.content


def comparar_transformacoes(vectorstore, consulta: str):
    """Compara diferentes t√©cnicas de transforma√ß√£o de consulta."""

    print(f"\n   Consulta Original: '{consulta}'")
    print("   " + "=" * 50)

    tecnicas = ["nenhuma", "hyde", "multi_query", "step_back", "decompor"]

    for tecnica in tecnicas:
        print(f"\n   üìå T√©cnica: {tecnica.upper()}")
        print("   " + "-" * 30)

        if tecnica == "nenhuma":
            docs = vectorstore.as_retriever(search_kwargs={"k": 3}).invoke(consulta)
        else:
            docs = recuperar_com_transformacao(vectorstore, consulta, tecnica, k=3)

        print(f"\n   Recuperados {len(docs)} documentos:")
        for i, doc in enumerate(docs[:3], 1):
            preview = doc.page_content[:100].replace('\n', ' ')
            print(f"   {i}. {preview}...")


def main():
    print("=" * 60)
    print("TRANSFORMA√á√ÉO DE CONSULTAS")
    print("=" * 60)

    if not CHROMA_DISPONIVEL:
        print("\nErro: chromadb √© necess√°rio para esta demonstra√ß√£o.")
        print("Instale com: pip install chromadb")
        return

    token_tracker.reset()

    # Carrega documentos e cria vector store
    print("\nüìö CARREGANDO DOCUMENTOS")
    print("-" * 40)

    documentos = carregar_documentos_exemplo()
    if not documentos:
        print("   Nenhum documento encontrado. Usando documentos de exemplo.")
        documentos = [
            Document(page_content="Machine learning √© um subconjunto de IA que permite sistemas aprenderem com dados."),
            Document(page_content="Redes neurais s√£o sistemas computacionais inspirados em redes neurais biol√≥gicas."),
            Document(page_content="Deep learning usa m√∫ltiplas camadas para extrair progressivamente features de alto n√≠vel."),
        ]

    print(f"   Carregados {len(documentos)} documentos")

    print("\n   Criando vector store...")
    vectorstore = criar_vectorstore(documentos, "demo_transformacao_query")
    print("   Vector store pronto!")

    # Demo 1: HyDE
    print("\n\nüîÆ HYDE - EMBEDDINGS DE DOCUMENTOS HIPOT√âTICOS")
    print("=" * 60)

    consulta1 = "Como redes neurais aprendem?"

    print(f"\n   Consulta: '{consulta1}'")
    print("\n   Gerando documento hipot√©tico...")
    doc_hyde = transformar_hyde(consulta1)
    print(f"\n   Documento Gerado:\n   {doc_hyde[:300]}...")

    docs = recuperar_com_transformacao(vectorstore, consulta1, "hyde", k=3)
    print("\n   Documentos recuperados com HyDE:")
    for i, doc in enumerate(docs[:3], 1):
        print(f"   {i}. {doc.page_content[:100]}...")

    # Demo 2: Multi-Query
    print("\n\nüîÑ TRANSFORMA√á√ÉO MULTI-QUERY")
    print("=" * 60)

    consulta2 = "Quais s√£o os benef√≠cios de usar transformers em NLP?"

    print(f"\n   Consulta: '{consulta2}'")
    print("\n   Gerando varia√ß√µes de consulta...")
    variacoes = transformar_multi_query(consulta2)
    print("\n   Varia√ß√µes:")
    for i, v in enumerate(variacoes, 1):
        print(f"   {i}. {v}")

    # Demo 3: Step-Back
    print("\n\n‚¨ÖÔ∏è STEP-BACK PROMPTING")
    print("=" * 60)

    consulta3 = "Por que o GPT-4 √†s vezes alucina fatos?"

    print(f"\n   Consulta: '{consulta3}'")
    print("\n   Gerando pergunta step-back...")
    step_back = transformar_step_back(consulta3)
    print(f"\n   Pergunta step-back: {step_back}")

    # Demo 4: Decomposi√ß√£o
    print("\n\nüî® DECOMPOSI√á√ÉO DE CONSULTA")
    print("=" * 60)

    consulta4 = "Como posso construir um sistema RAG que lida com m√∫ltiplos tipos de documentos e suporta mem√≥ria conversacional?"

    print(f"\n   Consulta: '{consulta4}'")
    print("\n   Decompondo em sub-perguntas...")
    sub_consultas = decompor_consulta(consulta4)
    print("\n   Sub-perguntas:")
    for i, q in enumerate(sub_consultas, 1):
        print(f"   {i}. {q}")

    # Demo 5: Pipeline completo com gera√ß√£o de resposta
    print("\n\nüéØ DEMONSTRA√á√ÉO DE PIPELINE COMPLETO")
    print("=" * 60)

    consulta5 = "O que √© machine learning e como √© usado?"

    print(f"\n   Consulta: '{consulta5}'")

    for tecnica in ["nenhuma", "hyde", "multi_query"]:
        print(f"\n   --- Usando {tecnica.upper()} ---")

        if tecnica == "nenhuma":
            docs = vectorstore.as_retriever(search_kwargs={"k": 3}).invoke(consulta5)
        else:
            docs = recuperar_com_transformacao(vectorstore, consulta5, tecnica, k=3)

        resposta = gerar_resposta(consulta5, docs)
        print(f"\n   Resposta: {resposta[:300]}...")

    # Melhores pr√°ticas
    print("\n\nüí° QUANDO USAR CADA T√âCNICA")
    print("-" * 40)
    print("""
   | T√©cnica       | Melhor Para                               |
   |---------------|-------------------------------------------|
   | HyDE          | Consultas com incompatibilidade de vocab  |
   | Multi-Query   | Consultas amb√≠guas ou vagas               |
   | Step-Back     | Perguntas espec√≠ficas precisando contexto |
   | Decomposi√ß√£o  | Perguntas complexas de m√∫ltiplas partes   |

   Dicas:
   - HyDE funciona melhor com consultas factuais, buscando conhecimento
   - Multi-Query ajuda quando voc√™ n√£o tem certeza da terminologia exata
   - Step-Back √© √∫til para perguntas "por que" e "como"
   - Decomposi√ß√£o lida bem com perguntas compostas

   Combine t√©cnicas para resultados ainda melhores!
    """)

    print_total_usage(token_tracker, "TOTAL - Transforma√ß√£o de Consultas")

    print("\nFim da demonstra√ß√£o de Transforma√ß√£o de Consultas")
    print("=" * 60)


if __name__ == "__main__":
    main()
