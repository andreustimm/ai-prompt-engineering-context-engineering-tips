"""
Zero-Shot Prompting

T√©cnica onde o modelo recebe uma tarefa sem exemplos pr√©vios.
O modelo usa apenas seu conhecimento pr√©-treinado para responder.

Casos de uso:
- Classifica√ß√£o de sentimento
- Tradu√ß√£o de texto
- Extra√ß√£o de entidades
- Perguntas e respostas simples
"""

import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from langchain_core.prompts import ChatPromptTemplate
from config import (
    get_llm,
    TokenUsage,
    extract_tokens_from_response,
    print_token_usage,
    print_total_usage
)

# Tracker global de tokens para este script
token_tracker = TokenUsage()


def classificar_sentimento(texto: str) -> str:
    """Classifica o sentimento de um texto sem exemplos."""
    llm = get_llm(temperature=0)

    prompt = ChatPromptTemplate.from_messages([
        ("system", "Voc√™ √© um analisador de sentimentos. Classifique o sentimento do texto como: POSITIVO, NEGATIVO ou NEUTRO. Responda apenas com a classifica√ß√£o."),
        ("user", "{texto}")
    ])

    chain = prompt | llm
    response = chain.invoke({"texto": texto})

    # Extrair e registrar tokens
    input_tokens, output_tokens = extract_tokens_from_response(response)
    token_tracker.add(input_tokens, output_tokens)
    print_token_usage(input_tokens, output_tokens)

    return response.content


def traduzir_texto(texto: str, idioma_destino: str) -> str:
    """Traduz texto para o idioma especificado."""
    llm = get_llm(temperature=0.3)

    prompt = ChatPromptTemplate.from_messages([
        ("system", "Voc√™ √© um tradutor profissional. Traduza o texto para {idioma}. Retorne apenas a tradu√ß√£o, sem explica√ß√µes."),
        ("user", "{texto}")
    ])

    chain = prompt | llm
    response = chain.invoke({"texto": texto, "idioma": idioma_destino})

    # Extrair e registrar tokens
    input_tokens, output_tokens = extract_tokens_from_response(response)
    token_tracker.add(input_tokens, output_tokens)
    print_token_usage(input_tokens, output_tokens)

    return response.content


def extrair_entidades(texto: str) -> str:
    """Extrai entidades nomeadas de um texto."""
    llm = get_llm(temperature=0)

    prompt = ChatPromptTemplate.from_messages([
        ("system", """Voc√™ √© um extrator de entidades nomeadas.
Extraia e liste as seguintes entidades do texto:
- PESSOA: nomes de pessoas
- LOCAL: lugares, cidades, pa√≠ses
- ORGANIZA√á√ÉO: empresas, institui√ß√µes
- DATA: datas e per√≠odos

Formato de sa√≠da:
PESSOA: [lista]
LOCAL: [lista]
ORGANIZA√á√ÉO: [lista]
DATA: [lista]"""),
        ("user", "{texto}")
    ])

    chain = prompt | llm
    response = chain.invoke({"texto": texto})

    # Extrair e registrar tokens
    input_tokens, output_tokens = extract_tokens_from_response(response)
    token_tracker.add(input_tokens, output_tokens)
    print_token_usage(input_tokens, output_tokens)

    return response.content


def resumir_texto(texto: str) -> str:
    """Resume um texto em poucas frases."""
    llm = get_llm(temperature=0.5)

    prompt = ChatPromptTemplate.from_messages([
        ("system", "Voc√™ √© um especialista em resumos. Resuma o texto a seguir em no m√°ximo 3 frases, mantendo as informa√ß√µes mais importantes."),
        ("user", "{texto}")
    ])

    chain = prompt | llm
    response = chain.invoke({"texto": texto})

    # Extrair e registrar tokens
    input_tokens, output_tokens = extract_tokens_from_response(response)
    token_tracker.add(input_tokens, output_tokens)
    print_token_usage(input_tokens, output_tokens)

    return response.content


def main():
    print("=" * 60)
    print("ZERO-SHOT PROMPTING - Demonstra√ß√£o")
    print("=" * 60)

    # Reset do tracker
    token_tracker.reset()

    # Exemplo 1: Classifica√ß√£o de Sentimento
    print("\nüìä CLASSIFICA√á√ÉO DE SENTIMENTO")
    print("-" * 40)

    textos_sentimento = [
        "Este produto √© incr√≠vel! Superou todas as minhas expectativas.",
        "P√©ssimo atendimento, nunca mais volto nessa loja.",
        "O pacote chegou hoje √†s 14h conforme previsto."
    ]

    for texto in textos_sentimento:
        print(f"\nTexto: {texto[:50]}...")
        resultado = classificar_sentimento(texto)
        print(f"Sentimento: {resultado}")

    # Exemplo 2: Tradu√ß√£o
    print("\n\nüåç TRADU√á√ÉO")
    print("-" * 40)

    texto_original = "Artificial intelligence is transforming how we work and live."
    print(f"\nOriginal: {texto_original}")
    traducao = traduzir_texto(texto_original, "portugu√™s brasileiro")
    print(f"Tradu√ß√£o: {traducao}")

    # Exemplo 3: Extra√ß√£o de Entidades
    print("\n\nüè∑Ô∏è EXTRA√á√ÉO DE ENTIDADES")
    print("-" * 40)

    texto_entidades = """
    Em mar√ßo de 2024, Elon Musk anunciou que a Tesla inaugurar√° uma nova
    f√°brica em S√£o Paulo. A parceria com o Governo do Estado prev√™
    investimentos de R$ 5 bilh√µes nos pr√≥ximos 3 anos.
    """

    print(f"\nTexto: {texto_entidades.strip()}")
    entidades = extrair_entidades(texto_entidades)
    print(f"\nEntidades extra√≠das:\n{entidades}")

    # Exemplo 4: Resumo
    print("\n\nüìù RESUMO DE TEXTO")
    print("-" * 40)

    texto_longo = """
    A intelig√™ncia artificial generativa est√° revolucionando diversos setores
    da economia global. Empresas de tecnologia investem bilh√µes em pesquisa
    e desenvolvimento de modelos de linguagem cada vez mais sofisticados.
    Ferramentas como ChatGPT, Claude e Gemini permitem que usu√°rios comuns
    realizem tarefas complexas de escrita, programa√ß√£o e an√°lise de dados.
    No entanto, especialistas alertam para os riscos associados ao uso
    irrespons√°vel dessas tecnologias, incluindo a dissemina√ß√£o de
    desinforma√ß√£o e preocupa√ß√µes com privacidade de dados.
    """

    print(f"\nTexto original: {texto_longo.strip()}")
    resumo = resumir_texto(texto_longo)
    print(f"\nResumo:\n{resumo}")

    # Exibir total de tokens
    print_total_usage(token_tracker, "TOTAL - Zero-Shot Prompting")

    print("\nFim da demonstra√ß√£o Zero-Shot")
    print("=" * 60)


if __name__ == "__main__":
    main()
